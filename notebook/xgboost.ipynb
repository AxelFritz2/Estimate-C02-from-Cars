{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.17",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "pip install xgboost"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-11-09T16:13:16.525155Z",
     "iopub.execute_input": "2023-11-09T16:13:16.525524Z",
     "iopub.status.idle": "2023-11-09T16:13:38.390956Z",
     "shell.execute_reply.started": "2023-11-09T16:13:16.525487Z",
     "shell.execute_reply": "2023-11-09T16:13:38.389475Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting xgboost\n  Downloading xgboost-2.0.1-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n\u001B[2K     \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m297.1/297.1 MB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/site-packages (from xgboost) (1.10.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from xgboost) (1.23.5)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-2.0.1\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\nNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, train_test_split \n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-09T16:14:44.246031Z",
     "iopub.execute_input": "2023-11-09T16:14:44.246472Z",
     "iopub.status.idle": "2023-11-09T16:14:44.457867Z",
     "shell.execute_reply.started": "2023-11-09T16:14:44.246440Z",
     "shell.execute_reply": "2023-11-09T16:14:44.456993Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:43:01.767957Z",
     "start_time": "2023-11-09T16:43:01.121301Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class data_prep"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "class DataPreparation:\n",
    "    def __init__(self, train, test, neural_networks = False, target = None):\n",
    "        self.train = train.copy()\n",
    "        self.test = test.copy()\n",
    "        self.target = target\n",
    "        self.neural_networks = neural_networks\n",
    "\n",
    "    def get_variable_correlation(self, variable):\n",
    "        correlation_vector = self.train[self.col_numericals].corr()[variable][:]\n",
    "        correlation_vector = np.abs(correlation_vector)\n",
    "        correlation_vector = correlation_vector.sort_values(ascending=False)[1:]\n",
    "\n",
    "        return (correlation_vector)\n",
    "\n",
    "    def get_nan_table(self):\n",
    "        nan_table = pd.DataFrame(columns=[\"Variable\", \"Pourcentages de valeurs manquantes\", \"Type\"])\n",
    "\n",
    "        for col in self.train:\n",
    "            pcentage = (self.train[col].isna().sum() / self.train.shape[0]) * 100\n",
    "            type = 'Num√©rique' if col in self.col_numericals else 'Cat√©gorielle'\n",
    "            nan_table.loc[len(nan_table)] = [col, pcentage, type]\n",
    "\n",
    "        nan_table = nan_table.sort_values(by=[\"Pourcentages de valeurs manquantes\"], ascending=False).reset_index(\n",
    "            drop=True)\n",
    "\n",
    "        return nan_table\n",
    "\n",
    "    def remove_train_nan(self):\n",
    "        percentage = (self.train.isna().sum() / self.train.shape[0]) * 100\n",
    "        self.columns_to_delete = [col for col in self.train.columns if\n",
    "                                  col != \"Electric range (km)\" and percentage[col] >= 40]\n",
    "        self.columns_to_delete.extend([\"r\", \"Status\", \"IT\", 'Date of registration', \"Mp\"])\n",
    "        self.train.drop(columns=self.columns_to_delete, inplace=True)\n",
    "        print(\"Valeurs manquantes du train supprim√©es ‚úÖ\")\n",
    "\n",
    "    def remove_test_nan(self):\n",
    "        self.test.drop(columns=self.columns_to_delete, inplace=True)\n",
    "        print(\"Valeurs manquantes du test supprim√©es ‚úÖ\")\n",
    "\n",
    "    def rename_columns(self):\n",
    "        self.train.columns = self.train.columns.str.replace(' ', '_')\n",
    "        self.test.columns = self.test.columns.str.replace(' ', '_')\n",
    "        print(\"Variables renomm√©es ‚úÖ\")\n",
    "\n",
    "    def get_type_list(self):\n",
    "        self.col_categoricals = self.train.select_dtypes(include=['object']).columns.to_list()\n",
    "        self.col_numericals = self.train.select_dtypes(exclude=[\"object\"]).columns.to_list()\n",
    "        self.col_numericals.remove(\"ID\")\n",
    "\n",
    "    def impute_by_mean(self, var_to_impute, var_mean):\n",
    "        mean_train = self.train.groupby(var_mean)[var_to_impute].transform('mean')\n",
    "        self.train[var_to_impute].fillna(mean_train, inplace=True)\n",
    "        self.train[var_to_impute].fillna(self.train[var_to_impute].mean(), inplace=True)\n",
    "\n",
    "        mean_test = self.test.groupby(var_mean)[var_to_impute].transform('mean')\n",
    "        self.test[var_to_impute].fillna(mean_test, inplace=True)\n",
    "        self.test[var_to_impute].fillna(self.test[var_to_impute].mean(), inplace=True)\n",
    "\n",
    "    def impute_train_test_numerical(self):\n",
    "\n",
    "        # Imputation of Electric range\n",
    "        self.train['Electric_range_(km)'].fillna(0, inplace=True)\n",
    "        self.test['Electric_range_(km)'].fillna(0, inplace=True)\n",
    "\n",
    "        # Imputation of Ec\n",
    "        self.train[\"ec_(cm3)\"].fillna(0, inplace=True)\n",
    "        self.test['ec_(cm3)'].fillna(0, inplace=True)\n",
    "\n",
    "        #Imputation of Mt\n",
    "        self.train.loc[self.train[\"Mt\"].isna(), \"Mt\"] = self.train.loc[self.train[\"Mt\"].isna()][\"m_(kg)\"]\n",
    "        self.train[\"Mt\"].fillna(self.train[\"Mt\"].mean(), inplace=True)\n",
    "\n",
    "        self.test.loc[self.test[\"Mt\"].isna(), \"Mt\"] = self.test.loc[self.test[\"Mt\"].isna()][\"m_(kg)\"]\n",
    "        self.test[\"Mt\"].fillna(self.test[\"Mt\"].mean(), inplace=True)\n",
    "\n",
    "        #Imputation of Fuel Consumption\n",
    "\n",
    "        self.train.loc[(self.train[\"Fuel_consumption_\"].isna()) & (self.train[\"Ft\"] == 'ELECTRIC'), \"Fuel_consumption_\"] = 0\n",
    "        self.test.loc[(self.test[\"Fuel_consumption_\"].isna()) & (self.test[\"Ft\"] == 'ELECTRIC'), \"Fuel_consumption_\"] = 0\n",
    "\n",
    "        self.impute_by_mean(\"Fuel_consumption_\", \"Cn\")\n",
    "\n",
    "        #Imputation of At1 (mm)\n",
    "        self.impute_by_mean(\"At1_(mm)\", \"Cn\")\n",
    "\n",
    "        # Imputation of At2 (mm)\n",
    "        self.impute_by_mean(\"At2_(mm)\", \"Cn\")\n",
    "\n",
    "        #Imputation of m (kg)\n",
    "        self.impute_by_mean(\"m_(kg)\", \"Cn\")\n",
    "\n",
    "        #Imputation of W\n",
    "        self.impute_by_mean(\"W_(mm)\", \"Cn\")\n",
    "\n",
    "        #Imputation of ep\n",
    "        self.impute_by_mean(\"ep_(KW)\", \"Cn\")\n",
    "\n",
    "        print(\"Valeurs manquantes num√©riques imput√©es ‚úÖ\")\n",
    "\n",
    "    def impute_train_test_categorical(self):\n",
    "        #Imputation of 'Cn'\n",
    "        mode_VFN_train = self.train.groupby('T')['Cn'].apply(\n",
    "            lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "        self.train['Cn'] = self.train['Cn'].fillna(self.train['T'].map(mode_VFN_train))\n",
    "\n",
    "        mode_VFN_test = self.test.groupby('T')['Cn'].apply(\n",
    "            lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "        self.test['Cn'] = self.test['Cn'].fillna(self.test['T'].map(mode_VFN_test))\n",
    "\n",
    "        # Impute 'Cn' par le mode de Cn si T est manquant\n",
    "        self.train['Cn'].fillna(self.train['Cn'].mode()[0], inplace=True)\n",
    "\n",
    "        self.test['Cn'].fillna(self.test['Cn'].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "        # Imputation of 'VFN'\n",
    "        mode_VFN_train = self.train.groupby('Cn')['VFN'].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "        self.train['VFN'] = self.train['VFN'].fillna(self.train['Cn'].map(mode_VFN_train))\n",
    "\n",
    "        mode_VFN_test = self.test.groupby('Cn')['VFN'].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "        self.test['VFN'] = self.test['VFN'].fillna(self.test['Cn'].map(mode_VFN_test))\n",
    "\n",
    "        # Impute 'VFN' with 'Va' if 'Cn' is missing\n",
    "        self.train['VFN'] = self.train.apply(\n",
    "            lambda row: row['Va'] if pd.isna(row['VFN']) and not pd.isna(row['Cn']) else row['VFN'],\n",
    "            axis=1)\n",
    "\n",
    "        self.test['VFN'] = self.test.apply(\n",
    "            lambda row: row['Va'] if pd.isna(row['VFN']) and not pd.isna(row['Cn']) else row['VFN'],\n",
    "            axis=1)\n",
    "\n",
    "        # Impute 'VFN' with mode of 'VFN' if both 'Cn' and 'Va' are missing\n",
    "        self.train['VFN'].fillna(self.train['VFN'].mode()[0], inplace=True)\n",
    "\n",
    "        self.test['VFN'].fillna(self.test['VFN'].mode()[0], inplace=True)\n",
    "\n",
    "        # Imputation des variables ayant moins de 1% de NaN\n",
    "        for col in ['Tan', 'T', 'Va', 'Ve', 'Mk', 'Ct', 'Fm']:\n",
    "            self.train[col].fillna(self.train[col].mode()[0],inplace=True)\n",
    "            self.test[col].fillna(self.train[col].mode()[0], inplace=True)\n",
    "\n",
    "        self.train.Ft = self.train.Ft.apply(lambda x: \"PETROL\" if x == \"UNKNOWN\" else x)\n",
    "        self.train[\"Ewltp_(g/km)\"] = self.train[\"Ewltp_(g/km)\"].apply(lambda x: 0 if x < 0 else x)\n",
    "        self.train.Ft = self.train.Ft.apply(lambda x: \"NG\" if x == \"NG-BIOMETHANE\" else x)\n",
    "        self.train.Ft = self.train.Ft.apply(\n",
    "            lambda x: \"ELECTRIC/HYDROGEN\" if x == \"HYDROGEN\" or x == \"ELECTRIC\" else x)\n",
    "        self.train.Ft = self.train.Ft.apply(\n",
    "            lambda x: \"HYBRID\" if x == \"PETROL/ELECTRIC\" or x == \"DIESEL/ELECTRIC\" else x)\n",
    "        self.train.Ft = self.train.Ft.apply(lambda x: \"BIOCARB\" if x == \"NG\" or x == \"E85\" or x == \"LPG\" else x)\n",
    "\n",
    "        self.test.Ft = self.test.Ft.apply(lambda x: \"PETROL\" if x == \"UNKNOWN\" else x)\n",
    "        self.test.Ft = self.test.Ft.apply(lambda x: \"NG\" if x == \"NG-BIOMETHANE\" else x)\n",
    "        self.test.Ft = self.test.Ft.apply(lambda x: \"ELECTRIC/HYDROGEN\" if x == \"HYDROGEN\" or x == \"ELECTRIC\" else x)\n",
    "        self.test.Ft = self.test.Ft.apply(\n",
    "            lambda x: \"HYBRID\" if x == \"PETROL/ELECTRIC\" or x == \"DIESEL/ELECTRIC\" else x)\n",
    "        self.test.Ft = self.test.Ft.apply(lambda x: \"BIOCARB\" if x == \"NG\" or x == \"E85\" or x == \"LPG\" else x)\n",
    "\n",
    "        print(\"Valeurs manquantes cat√©gorielles imput√©es ‚úÖ\")\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.remove_train_nan()\n",
    "        self.remove_test_nan()\n",
    "        self.rename_columns()\n",
    "        self.get_type_list()\n",
    "        self.impute_train_test_numerical()\n",
    "        self.impute_train_test_categorical()\n",
    "\n",
    "        return self.train, self.test\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-09T16:14:56.370934Z",
     "iopub.execute_input": "2023-11-09T16:14:56.371360Z",
     "iopub.status.idle": "2023-11-09T16:14:56.415075Z",
     "shell.execute_reply.started": "2023-11-09T16:14:56.371330Z",
     "shell.execute_reply": "2023-11-09T16:14:56.414147Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:43:03.533709Z",
     "start_time": "2023-11-09T16:43:03.527574Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import des donn√©es "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#df_train = pd.read_csv('/kaggle/input/estimate-co2-emissions-from-cars/train.csv')\n",
    "#df_test = pd.read_csv('/kaggle/input/estimate-co2-emissions-from-cars/test.csv')\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-09T16:17:25.496028Z",
     "iopub.execute_input": "2023-11-09T16:17:25.496480Z",
     "iopub.status.idle": "2023-11-09T16:18:26.967951Z",
     "shell.execute_reply.started": "2023-11-09T16:17:25.496449Z",
     "shell.execute_reply": "2023-11-09T16:18:26.966829Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:44:10.421564Z",
     "start_time": "2023-11-09T16:43:32.399942Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/12/33kq22951gn7jfpz3d32v_qc0000gn/T/ipykernel_3800/358514020.py:5: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv('../data/train.csv')\n",
      "/var/folders/12/33kq22951gn7jfpz3d32v_qc0000gn/T/ipykernel_3800/358514020.py:6: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv('../data/test.csv')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pr√©paration des donn√©es"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "dataprep = DataPreparation(df_train, df_test)\n",
    "clean_train, clean_test = train_clean, test_clean = dataprep.prepare_data()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-09T16:18:29.767768Z",
     "iopub.execute_input": "2023-11-09T16:18:29.768865Z",
     "iopub.status.idle": "2023-11-09T16:20:54.101513Z",
     "shell.execute_reply.started": "2023-11-09T16:18:29.768815Z",
     "shell.execute_reply": "2023-11-09T16:20:54.100251Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:49:45.287779Z",
     "start_time": "2023-11-09T16:47:58.311857Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes du train supprim√©es ‚úÖ\n",
      "Valeurs manquantes du test supprim√©es ‚úÖ\n",
      "Variables renomm√©es ‚úÖ\n",
      "Valeurs manquantes num√©riques imput√©es ‚úÖ\n",
      "Valeurs manquantes cat√©gorielles imput√©es ‚úÖ\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "clean_train_avant_encoding = clean_train.copy()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-09T16:20:54.103475Z",
     "iopub.execute_input": "2023-11-09T16:20:54.103842Z",
     "iopub.status.idle": "2023-11-09T16:20:59.004945Z",
     "shell.execute_reply.started": "2023-11-09T16:20:54.103813Z",
     "shell.execute_reply": "2023-11-09T16:20:59.003686Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One Hot Encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "ft_column = clean_train['Ft']\n",
    "\n",
    "# Appliquez l'encodage one-hot √† la colonne 'Ft' avec un pr√©fixe\n",
    "encoded_ft = pd.get_dummies(ft_column, prefix='Ft', dtype=int)\n",
    "\n",
    "# Ajoutez les colonnes encod√©es √† votre DataFrame\n",
    "clean_train = pd.concat([clean_train, encoded_ft], axis=1)\n",
    "\n",
    "# Supprimez la colonne originale 'Ft' si vous le souhaitez\n",
    "clean_train = clean_train.drop(columns=['Ft'])\n",
    "\n",
    "ft_column = clean_train['Fm']\n",
    "\n",
    "# Appliquez l'encodage one-hot √† la colonne 'Ft' avec un pr√©fixe\n",
    "encoded_ft = pd.get_dummies(ft_column, prefix='Fm', dtype=int)\n",
    "\n",
    "# Ajoutez les colonnes encod√©es √† votre DataFrame\n",
    "clean_train = pd.concat([clean_train, encoded_ft], axis=1)\n",
    "\n",
    "# Supprimez la colonne originale 'Ft' si vous le souhaitez\n",
    "clean_train = clean_train.drop(columns=['Fm'])\n",
    "\n",
    "\n",
    "# -----------------------------------------------------Pareil pour test -----------------------------------------------------\n",
    "\n",
    "ft_column = clean_test['Ft']\n",
    "\n",
    "# Appliquez l'encodage one-hot √† la colonne 'Ft' avec un pr√©fixe\n",
    "encoded_ft = pd.get_dummies(ft_column, prefix='Ft', dtype=int)\n",
    "\n",
    "# Ajoutez les colonnes encod√©es √† votre DataFrame\n",
    "clean_test = pd.concat([clean_test, encoded_ft], axis=1)\n",
    "\n",
    "# Supprimez la colonne originale 'Ft' si vous le souhaitez\n",
    "clean_test = clean_test.drop(columns=['Ft'])\n",
    "\n",
    "ft_column = clean_test['Fm']\n",
    "\n",
    "# Appliquez l'encodage one-hot √† la colonne 'Ft' avec un pr√©fixe\n",
    "encoded_ft = pd.get_dummies(ft_column, prefix='Fm', dtype=int)\n",
    "\n",
    "# Ajoutez les colonnes encod√©es √† votre DataFrame\n",
    "clean_test = pd.concat([clean_test, encoded_ft], axis=1)\n",
    "\n",
    "# Supprimez la colonne originale 'Ft' si vous le souhaitez\n",
    "clean_test = clean_test.drop(columns=['Fm'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-09T16:20:59.006158Z",
     "iopub.execute_input": "2023-11-09T16:20:59.006438Z",
     "iopub.status.idle": "2023-11-09T16:21:07.107535Z",
     "shell.execute_reply.started": "2023-11-09T16:20:59.006413Z",
     "shell.execute_reply": "2023-11-09T16:21:07.106253Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:50:10.626496Z",
     "start_time": "2023-11-09T16:49:58.400207Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Impact Encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Imaginons que `clean_train` est votre jeu de donn√©es d'entra√Ænement et `clean_test` est votre jeu de donn√©es de test\n",
    "\n",
    "# Initialiser un dictionnaire pour stocker les dictionnaires d'impact de chaque caract√©ristique cat√©gorielle\n",
    "impact_dicts = {}\n",
    "\n",
    "for categorical_feature in ['T', 'Man', 'VFN', 'Tan', 'Cn', 'Mk', 'Va']:\n",
    "    # Calculer la moyenne de la valeur cible pour chaque cat√©gorie\n",
    "    category_means = clean_train.groupby(categorical_feature)['Ewltp_(g/km)'].mean()\n",
    "\n",
    "    # Calculer la diff√©rence de la moyenne de la valeur cible entre chaque cat√©gorie et la moyenne globale\n",
    "    category_impacts = category_means - category_means.mean()\n",
    "\n",
    "    # Stocker l'impact de la cat√©gorie dans un dictionnaire\n",
    "    impact_dicts[categorical_feature] = category_impacts.to_dict()\n",
    "\n",
    "    # Encoder la caract√©ristique cat√©gorielle dans le jeu de donn√©es d'entra√Ænement\n",
    "    clean_train['encoded_' + categorical_feature] = clean_train[categorical_feature].map(impact_dicts[categorical_feature])\n",
    "\n",
    "    # Supprimer la caract√©ristique cat√©gorielle originale du jeu de donn√©es d'entra√Ænement\n",
    "    clean_train.drop(columns=categorical_feature, inplace=True)\n",
    "\n",
    "# Appliquer les m√™mes encodages aux donn√©es de test\n",
    "for categorical_feature in ['T', 'Man', 'VFN', 'Tan', 'Cn', 'Mk', 'Va']:\n",
    "    # Si une valeur dans le jeu de donn√©es de test n'a pas √©t√© vue dans le jeu de donn√©es d'entra√Ænement,\n",
    "    # on peut d√©cider de la remplacer par 0 ou la moyenne globale des impacts calcul√©s sur le jeu de donn√©es d'entra√Ænement\n",
    "    default_impact = impact_dicts[categorical_feature].get(\"Default\", 0)\n",
    "\n",
    "    # Encoder la caract√©ristique cat√©gorielle dans le jeu de donn√©es de test\n",
    "    clean_test['encoded_' + categorical_feature] = clean_test[categorical_feature].apply(lambda x: impact_dicts[categorical_feature].get(x, default_impact))\n",
    "\n",
    "    # Supprimer la caract√©ristique cat√©gorielle originale du jeu de donn√©es de test\n",
    "    clean_test.drop(columns=categorical_feature, inplace=True)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-09T16:39:06.864718Z",
     "iopub.execute_input": "2023-11-09T16:39:06.865277Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:50:45.595190Z",
     "start_time": "2023-11-09T16:50:23.676157Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ID', 'Country', 'Mh', 'Ve', 'Ct', 'Cr', 'm_(kg)', 'Mt', 'Ewltp_(g/km)',\n       'W_(mm)', 'At1_(mm)', 'At2_(mm)', 'ec_(cm3)', 'ep_(KW)',\n       'Fuel_consumption_', 'Electric_range_(km)', 'Ft_BIOCARB', 'Ft_DIESEL',\n       'Ft_ELECTRIC/HYDROGEN', 'Ft_HYBRID', 'Ft_PETROL', 'Fm_B', 'Fm_E',\n       'Fm_F', 'Fm_H', 'Fm_M', 'Fm_P', 'encoded_T', 'encoded_Man',\n       'encoded_VFN', 'encoded_Tan', 'encoded_Cn', 'encoded_Mk', 'encoded_Va'],\n      dtype='object')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:51:35.508516Z",
     "start_time": "2023-11-09T16:51:35.497804Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ID', 'Country', 'Mh', 'Ve', 'Ct', 'Cr', 'm_(kg)', 'Mt', 'W_(mm)',\n       'At1_(mm)', 'At2_(mm)', 'ec_(cm3)', 'ep_(KW)', 'Fuel_consumption_',\n       'Electric_range_(km)', 'Ft_BIOCARB', 'Ft_DIESEL',\n       'Ft_ELECTRIC/HYDROGEN', 'Ft_HYBRID', 'Ft_PETROL', 'Fm_B', 'Fm_E',\n       'Fm_F', 'Fm_H', 'Fm_M', 'Fm_P', 'encoded_T', 'encoded_Man',\n       'encoded_VFN', 'encoded_Tan', 'encoded_Cn', 'encoded_Mk', 'encoded_Va'],\n      dtype='object')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:51:47.726721Z",
     "start_time": "2023-11-09T16:51:47.722808Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Standardization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the StandardScaler to the data\n",
    "data_scaled = scaler.fit_transform(clean_train.select_dtypes(include = 'float').drop(columns ='Ewltp_(g/km)'))\n",
    "test_scaled = scaler.transform(clean_test.select_dtypes(include = 'float'))\n",
    "\n",
    "# Save the scaled data\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=clean_train.select_dtypes(include = 'float').drop(columns ='Ewltp_(g/km)').columns)\n",
    "clean_train_scaled = pd.concat([data_scaled,clean_train.select_dtypes(exclude = 'float')], axis = 1)\n",
    "\n",
    "# Same with test\n",
    "test_scaled = pd.DataFrame(test_scaled, columns=clean_test.select_dtypes(include = 'float').columns)\n",
    "clean_test_scaled = pd.concat([test_scaled,clean_test.select_dtypes(exclude = 'float')], axis = 1)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-09T16:24:38.703462Z",
     "iopub.execute_input": "2023-11-09T16:24:38.703942Z",
     "iopub.status.idle": "2023-11-09T16:24:49.560221Z",
     "shell.execute_reply.started": "2023-11-09T16:24:38.703904Z",
     "shell.execute_reply": "2023-11-09T16:24:49.559004Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:52:34.262018Z",
     "start_time": "2023-11-09T16:52:26.377270Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train test split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "X = clean_train_scaled.select_dtypes(exclude = 'object')\n",
    "y = clean_train['Ewltp_(g/km)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-09T16:24:55.947473Z",
     "iopub.execute_input": "2023-11-09T16:24:55.947902Z",
     "iopub.status.idle": "2023-11-09T16:24:59.182734Z",
     "shell.execute_reply.started": "2023-11-09T16:24:55.947870Z",
     "shell.execute_reply": "2023-11-09T16:24:59.181492Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:52:52.683077Z",
     "start_time": "2023-11-09T16:52:49.508849Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_ESTIMATORS = 100\n",
    "MAX_DEPTH = 20\n",
    "N_ITERATION = 100\n",
    "\n",
    "# Cr√©ez un mod√®le XGBoost avec les m√™mes param√®tres\n",
    "model_xgboost = xgb.XGBRegressor(n_estimators=N_ESTIMATORS, max_depth=MAX_DEPTH, random_state=10)\n",
    "\n",
    "print(\"lancement !\")\n",
    "\n",
    "# Cr√©ez un objet KFold en sp√©cifiant le nombre de plis (n_splits)\n",
    "n_splits = 2  # Vous pouvez ajuster le nombre de plis selon vos besoins\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "\n",
    "# Initialisez une liste pour stocker les scores de MAE\n",
    "mae_scores = []\n",
    "\n",
    "# Effectuez la validation crois√©e\n",
    "for train_index, test_index in kfold.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Fit en cours\n",
    "    print(\"fit en cours... ‚è≥\")\n",
    "    model_xgboost.fit(X_train_fold, y_train_fold)\n",
    "    print(\"fit termin√© üéâ\")\n",
    "\n",
    "    # Predict en cours\n",
    "    print(\"predict en cours... ‚è≥\")\n",
    "    y_pred_fold = model_xgboost.predict(X_test_fold)\n",
    "    print(\"predict effectu√© üéâ\")\n",
    "\n",
    "    # Calcul de la MAE pour ce pli\n",
    "    mae_fold = mean_absolute_error(y_test_fold, y_pred_fold)\n",
    "    mae_scores.append(mae_fold)\n",
    "    print(f\"MAE {mae_fold}\")\n",
    "\n",
    "# Calcul de la moyenne des scores MAE de tous les plis\n",
    "mae_mean = np.mean(mae_scores)\n",
    "print(f\"MAE moyenne de XGBoost avec {n_splits} plis : {mae_mean}\")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-09T16:25:06.453593Z",
     "iopub.execute_input": "2023-11-09T16:25:06.454658Z",
     "iopub.status.idle": "2023-11-09T16:29:26.820282Z",
     "shell.execute_reply.started": "2023-11-09T16:25:06.454616Z",
     "shell.execute_reply": "2023-11-09T16:29:26.819281Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:55:37.845881Z",
     "start_time": "2023-11-09T16:52:52.684498Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lancement !\n",
      "fit en cours... ‚è≥\n",
      "fit termin√© üéâ\n",
      "predict en cours... ‚è≥\n",
      "predict effectu√© üéâ\n",
      "MAE 2.8705692969797303\n",
      "fit en cours... ‚è≥\n",
      "fit termin√© üéâ\n",
      "predict en cours... ‚è≥\n",
      "predict effectu√© üéâ\n",
      "MAE 2.867204048031602\n",
      "MAE moyenne de XGBoost avec 2 plis : 2.868886672505666\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "prediction = model_xgboost.predict(clean_test_scaled.select_dtypes(exclude = 'object'))\n",
    "\n",
    "submission = clean_test_scaled[[\"ID\"]].copy()  \n",
    "submission[\"Ewltp (g/km)\"] = prediction"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T16:57:06.400786Z",
     "start_time": "2023-11-09T16:57:00.097029Z"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "submission.to_csv(\"../data/sample_submission_xgb.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T16:57:45.962657Z",
     "start_time": "2023-11-09T16:57:43.362609Z"
    }
   }
  }
 ]
}
